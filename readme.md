# AnyLLM-to-VLM ðŸš€

Turn any text-only LLM into a Vision-Language Model through efficient training.

## Features
- ðŸ”Œ Plug-and-play adapter for existing LLMs
- ðŸŽ¯ Minimal training required  
- ðŸ“· Support for CLIP and other vision encoders
- ðŸ¦™ Compatible with popular LLMs (Qwen, LLaMA, etc.)

## Quick Start

Follow these steps to get started quickly:

1. **Clone the Repository**  
   Clone the repository to your local machine:
   ```bash
   git clone https://github.com/sunshine-JLU/AnyLLM-to-VLM.git

   cd AnyLLM-to-VLM
