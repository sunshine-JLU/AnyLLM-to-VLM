model:
  vision_model_type: "clip"
  vision_model_path: "./models/clip-vit-base-patch16"  # 
  freeze_vision_encoder: true
  vision_layers_to_unfreeze: 0
  
  language_model_type: "qwen3"  # 支持所有Qwen系列模型（0.5B, 0.6B, 1.5B, 2B, 7B等）
  language_model_path: "./models/Qwen2.5-0.5B-Instruct"
  freeze_language_model: false
  language_layers_to_unfreeze: 2  # 解冻后2层transformer层进行训练，设置为0则只训练投影层、embedding和输出层
  
  projection_type: "deep_mlp"  # 使用深层MLP
  projection_hidden_dim: 3072
  projection_activation: "gelu"
  projection_dropout: 0.1
  projection_layernorm: true
  
  image_special_token: "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
  
  vision_processor_type: "patch_insert"  # 'patch_insert' (直接插入所有patch) 或 'mean_pooling' (平均pooling后插入)
  
  use_bfloat16: true
  max_seq_length: 640
  
  # LoRA配置（可选，显存占用大时可启用）
  use_lora: true  # 设置为 true 启用LoRA训练
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  lora_target_modules: null  # 如果为null，将自动选择（通常为 ["q_proj", "k_proj", "v_proj", "o_proj"]）

training:
  stage: "pretrain"
  batch_size: 22
  num_epochs: 1
  # 参考LLaVA：预训练阶段学习率设置
  # LLaVA通常使用2e-3，但如果遇到训练不稳定或幻觉严重，可以降低到1e-3或5e-4
  learning_rate: 1e-3  # 使用中等学习率，平衡训练速度和稳定性
  warmup_steps: 1000
  weight_decay: 0.01
  grad_clip: 1.0
  
  data_path: "./dataset/pretrain_data.parquet"
  max_seq_length: 640
  
  accumulation_steps: 2
  use_mixed_precision: true
  mixed_precision_dtype: "bfloat16"
  
  save_dir: "./checkpoints/pretrain"
  save_every_n_steps: 1000
  log_every_n_steps: 100
  eval_every_n_steps: null
  
  # 分布式训练配置
  use_ddp: true  # 使用DDP（Distributed Data Parallel）
  use_fsdp: false  # 使用FSDP（Fully Sharded Data Parallel），可以更有效地利用显存
  # FSDP配置（仅在use_fsdp=true时生效）
  fsdp_sharding_strategy: "FULL_SHARD"  # "FULL_SHARD", "SHARD_GRAD_OP", "NO_SHARD", "HYBRID_SHARD"
  fsdp_cpu_offload: false  # 是否将参数offload到CPU（可以进一步节省显存，但会降低速度）
  fsdp_sync_module_states: true  # 是否同步模块状态
  fsdp_forward_prefetch: false  # 是否启用前向预取（可以加速，但可能增加显存）
  fsdp_use_orig_params: true  # 使用原始参数（推荐，与混合精度兼容）
  num_workers: 32
