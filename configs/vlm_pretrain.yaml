model:
  vision_model_type: "siglip"
  vision_model_path: "./models/siglip-so400m-patch14-384"  # 
  freeze_vision_encoder: true
  vision_layers_to_unfreeze: 0
  
  language_model_type: "qwen3"  # 支持所有Qwen系列模型（0.5B, 0.6B, 1.5B, 2B, 7B等）
  language_model_path: "./models/Qwen2.5-0.5B-Instruct"
  freeze_language_model: false
  language_layers_to_unfreeze: 2  # 解冻后2层transformer层进行训练，设置为0则只训练投影层、embedding和输出层
  
  projection_type: "deep_mlp"  # 使用深层MLP
  projection_hidden_dim: 3072
  projection_activation: "gelu"
  projection_dropout: 0.1
  projection_layernorm: true
  
  image_special_token: "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
  
  vision_processor_type: "patch_insert"  # 'patch_insert' (直接插入所有patch) 或 'mean_pooling' (平均pooling后插入)
  
  use_bfloat16: true
  max_seq_length: 640
  
  # LoRA配置（可选，显存占用大时可启用）
  use_lora: true  # 设置为 true 启用LoRA训练
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  lora_target_modules: null  # 如果为null，将自动选择（通常为 ["q_proj", "k_proj", "v_proj", "o_proj"]）

training:
  stage: "pretrain"
  batch_size: 64
  num_epochs: 1
  # 参考LLaVA：预训练阶段学习率设置
  # LLaVA通常使用2e-3，但如果遇到训练不稳定或幻觉严重，可以降低到1e-3或5e-4
  learning_rate: 1e-3  # 使用中等学习率，平衡训练速度和稳定性
  warmup_steps: 1000
  weight_decay: 0.01
  grad_clip: 1.0
  
  data_path: "./dataset/pretrain_data.parquet"
  max_seq_length: 640
  
  accumulation_steps: 2
  use_mixed_precision: true
  mixed_precision_dtype: "bfloat16"
  
  save_dir: "./checkpoints/pretrain"
  save_every_n_steps: 1000
  log_every_n_steps: 100
  eval_every_n_steps: null
  
  use_ddp: false
  num_workers: 2
