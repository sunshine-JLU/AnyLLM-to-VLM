model:
  vision_model_type: "clip"
  vision_model_path: "./models/clip-vit-base-patch16"
  freeze_vision_encoder: true
  vision_layers_to_unfreeze: 0
  
  language_model_type: "qwen3"  # 使用 'qwen3' 或 'qwen' 都可以，支持所有Qwen3系列模型（0.5B, 0.6B, 1.5B, 2B, 7B等）
  language_model_path: "./models/Qwen3-0.6B"
  freeze_language_model: false
  language_layers_to_unfreeze: 2  # 解冻后2层transformer层进行训练，设置为0则只训练投影层、embedding和输出层
  
  projection_type: "mlp"
  projection_hidden_dim: 3072
  projection_activation: "gelu"
  projection_dropout: 0.1
  projection_layernorm: true
  
  image_special_token: "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
  
  use_bfloat16: true
  max_seq_length: 640

training:
  stage: "pretrain"
  batch_size: 10
  num_epochs: 1
  learning_rate: 4e-4
  warmup_steps: 1000
  weight_decay: 0.01
  grad_clip: 1.0
  
  data_path: "./dataset/pretrain_data.parquet"
  max_seq_length: 640
  
  accumulation_steps: 1
  use_mixed_precision: true
  mixed_precision_dtype: "bfloat16"
  
  save_dir: "./checkpoints/pretrain"
  save_every_n_steps: 1000
  log_every_n_steps: 100
  eval_every_n_steps: null
  
  use_ddp: false
  num_workers: 1
