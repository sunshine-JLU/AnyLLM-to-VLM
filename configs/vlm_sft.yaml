# 模型配置
model:
  vision_model_type: "clip"
  vision_model_path: "./models/clip-vit-base-patch16"
  freeze_vision_encoder: true
  vision_layers_to_unfreeze: 0
  
  language_model_type: "qwen3"  # 使用 'qwen3' 或 'qwen' 都可以，支持所有Qwen3系列模型（0.5B, 0.6B, 1.5B, 2B, 7B等）
  language_model_path: "./models/Qwen2.5-0.5B-Instruct"
  freeze_language_model: false
  language_layers_to_unfreeze: 0  # SFT阶段可以解冻更多层，或设置为0只训练投影层和输出层
  
  projection_type: "mlp"
  projection_hidden_dim: 3072
  projection_activation: "gelu"
  projection_dropout: 0.1
  projection_layernorm: true
  
  image_special_token: "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
  image_token_ids: null
  
  vision_processor_type: "patch_insert"  # 'patch_insert' (直接插入所有patch) 或 'mean_pooling' (平均pooling后插入)
  
  use_bfloat16: true
  max_seq_length: 1536
  
  # LoRA配置（可选，显存占用大时可启用）
  use_lora: false  # 设置为 true 启用LoRA训练
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  lora_target_modules: null  # 如果为null，将自动选择（通常为 ["q_proj", "k_proj", "v_proj", "o_proj"]）

# 训练配置
training:
  stage: "sft"
  batch_size: 2
  num_epochs: 2
  learning_rate: 1e-5
  warmup_steps: 200
  weight_decay: 0.01
  grad_clip: 1.0
  
  data_path: "./dataset/sft_data.parquet"
  max_seq_length: 1536
  
  accumulation_steps: 4
  use_mixed_precision: true
  mixed_precision_dtype: "bfloat16"
  
  save_dir: "./checkpoints/sft"
  save_every_n_steps: 500
  log_every_n_steps: 50
  eval_every_n_steps: 200
  
  use_ddp: false
  num_workers: 8
