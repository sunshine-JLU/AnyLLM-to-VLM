# 模型配置
model:
  vision_model_type: "clip"
  vision_model_path: "./clip-vit-base-patch16"
  freeze_vision_encoder: true
  vision_layers_to_unfreeze: 0
  
  language_model_type: "qwen"
  language_model_path: "./Qwen3-0.6B"
  freeze_language_model: false
  language_layers_to_unfreeze: 0
  
  projection_type: "mlp"
  projection_hidden_dim: 3072
  projection_activation: "gelu"
  projection_dropout: 0.1
  projection_layernorm: true
  
  image_special_token: "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
  image_token_ids: null
  
  use_bfloat16: true
  max_seq_length: 1536

# 训练配置
training:
  stage: "sft"
  batch_size: 16
  num_epochs: 2
  learning_rate: 1e-5
  warmup_steps: 200
  weight_decay: 0.01
  grad_clip: 1.0
  
  data_path: "../dataset/sft_data.parquet"
  max_seq_length: 1536
  
  accumulation_steps: 1
  use_mixed_precision: true
  mixed_precision_dtype: "bfloat16"
  
  save_dir: "./checkpoints/sft"
  save_every_n_steps: 500
  log_every_n_steps: 50
  eval_every_n_steps: 200
  
  use_ddp: false
  num_workers: 8